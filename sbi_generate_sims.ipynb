{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!export JAX_PLATFORMS=cpu\n",
    "import tszpower\n",
    "from classy_sz import Class as Class_sz\n",
    "import jax\n",
    "import jax.numpy as jnp \n",
    "import numpy as np\n",
    "import torch\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import NPE\n",
    "from sbi.inference import NLE\n",
    "from sbi.inference import NRE_A\n",
    "# from sbi.inference import FMPE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define parameters\n",
    "allpars = {\n",
    "    # 'output': '',\n",
    "    'omega_b': 0.0225,\n",
    "    'omega_cdm': 0.12,\n",
    "    'H0': 67.66,\n",
    "    'tau_reio': 0.0561,\n",
    "    'ln10^{10}A_s': 3.0,\n",
    "    'n_s': 0.9665,\n",
    "    'M_min': 1e10,\n",
    "    'M_max': 3.5e15,\n",
    "    # 'ell_min': 2,\n",
    "    # 'ell_max': 8000,\n",
    "    # 'dlogell': 0.1,\n",
    "    'z_min': 5e-3,\n",
    "    'z_max': 3.0,\n",
    "    'P0GNFW': 8.130,\n",
    "    'c500': 1.156,\n",
    "    'gammaGNFW': 0.3292,\n",
    "    'alphaGNFW': 1.0620,\n",
    "    'betaGNFW': 5.4807,\n",
    "    'B': 1.0,\n",
    "    # \"cosmo_model\": 1, # use mnu-lcdm emulators\n",
    "    'jax': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tszpower.classy_sz.set(allpars)\n",
    "tszpower.classy_sz.compute_class_szfast()\n",
    "# relavant_pars = tszpower.classy_sz.get_all_relevant_params(allpars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(theta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Simulator that wraps tszpower.compute_Cl_yy_total.\n",
    "    \n",
    "    The input `theta` is expected to be a torch.Tensor of shape (batch, 9)\n",
    "    with columns ordered as:\n",
    "      [logA, omega_b, omega_cdm, H0, n_s, B, A_cib, A_rs, A_ir]\n",
    "      \n",
    "    This function uses the torch tensor directly by converting each element\n",
    "    to a Python float, calls the tszpower simulator, and then returns a torch.Tensor.\n",
    "    \"\"\"\n",
    "    batch_size = theta.shape[0]\n",
    "    \n",
    "    # Generate a base key and split it for each simulation.\n",
    "    base_key = jax.random.PRNGKey(2837)\n",
    "    keys = jax.random.split(base_key, batch_size)\n",
    "    # print(keys)\n",
    "    \n",
    "    sim_list = []\n",
    "    for i in range(batch_size):\n",
    "        # Extract each parameter as a Python float\n",
    "        logA      = float(theta[i, 0])\n",
    "        omega_b   = float(theta[i, 1])\n",
    "        omega_cdm = float(theta[i, 2])\n",
    "        H0        = float(theta[i, 3])\n",
    "        n_s       = float(theta[i, 4])\n",
    "        B         = float(theta[i, 5])\n",
    "        A_cib     = float(theta[i, 6])\n",
    "        A_rs      = float(theta[i, 7])\n",
    "        A_ir      = float(theta[i, 8])\n",
    "        \n",
    "        # Call your tszpower simulator (which uses JAX internally)\n",
    "        sim_i = tszpower.compute_Cl_yy_total(\n",
    "            logA,\n",
    "            omega_b,\n",
    "            omega_cdm,\n",
    "            H0,\n",
    "            n_s,\n",
    "            B,\n",
    "            A_cib,\n",
    "            A_rs,\n",
    "            A_ir,\n",
    "            keys[i],\n",
    "            params_values_dict=allpars,  # your global parameter dictionary\n",
    "            n_realizations=1\n",
    "        )\n",
    "        # Convert the returned JAX array to a NumPy array and then to a torch.Tensor\n",
    "        sim_torch = torch.tensor(np.array(sim_i), dtype=torch.float32)\n",
    "        sim_list.append(sim_torch)\n",
    "    \n",
    "    # Stack the results to form a tensor of shape (batch, n_ell)\n",
    "    return torch.stack(sim_list, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Prior ---\n",
    "# We need a 9-dimensional prior (one for each free parameter).\n",
    "# Order: [logA, omega_b, omega_cdm, H0, n_s, B, A_cib, A_rs, A_ir]\n",
    "\n",
    "low = torch.tensor([2.5,   0.02,  0.11, 55.,  0.94, 1.0, 0.0, 0.0, 0.0])\n",
    "high = torch.tensor([3.5, 0.025, 0.13, 90.,  1.0,  2.0, 5.0, 5.0, 5.0])\n",
    "prior = BoxUniform(low=low, high=high)\n",
    "\n",
    "# Process the prior.\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "# Process the simulator to ensure it returns batched outputs and torch.Tensors.\n",
    "simulator = process_simulator(simulator, prior, prior_returns_numpy)\n",
    "# Check consistency.\n",
    "check_sbi_inputs(simulator, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta is tensor([[3.0176, 0.0240, 0.1130,  ..., 4.3077, 0.0568, 2.9076],\n",
      "        [2.8796, 0.0232, 0.1132,  ..., 4.5891, 0.5272, 4.2974],\n",
      "        [3.0101, 0.0229, 0.1260,  ..., 0.1043, 0.6794, 2.3754],\n",
      "        ...,\n",
      "        [3.4286, 0.0241, 0.1198,  ..., 0.3638, 1.0195, 0.9560],\n",
      "        [2.9786, 0.0248, 0.1265,  ..., 1.9338, 0.2506, 1.4157],\n",
      "        [3.2569, 0.0204, 0.1121,  ..., 3.0460, 3.2573, 3.1324]])\n",
      "x is tensor([[ 9.1051e-03,  2.6906e-03,  5.3921e-03,  ...,  2.2004e+00,\n",
      "          3.2241e+00,  4.3651e+00],\n",
      "        [-1.7084e-03,  6.2439e-03, -6.2991e-03,  ...,  2.5262e+00,\n",
      "          3.7055e+00,  5.1380e+00],\n",
      "        [ 1.0130e-02,  1.6295e-02,  2.6257e-02,  ...,  1.6339e+00,\n",
      "          2.1461e+00,  2.8643e+00],\n",
      "        ...,\n",
      "        [-8.4075e-04,  4.8074e-04,  1.2115e-02,  ...,  1.0310e+00,\n",
      "          1.4078e+00,  1.8825e+00],\n",
      "        [ 7.0748e-03,  1.7674e-02,  4.2691e-02,  ...,  2.2044e+00,\n",
      "          2.9021e+00,  3.6707e+00],\n",
      "        [ 1.2858e-02, -4.5128e-03,  4.6135e-03,  ...,  2.7001e+00,\n",
      "          3.7699e+00,  5.0909e+00]])\n",
      "x shape is torch.Size([10000, 18])\n"
     ]
    }
   ],
   "source": [
    "theta = prior.sample((10000,))\n",
    "x = simulator(theta)\n",
    "print(\"theta is\", theta)\n",
    "print(\"x is\", x)\n",
    "print(\"x shape is\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 theta（假设 prior.sample() 生成的 theta 已经是 tensor [2000, D]\n",
    "torch.save(theta, 'theta_10ksamps.pt')  # 原生 PyTorch 格式\n",
    "# 或保存为可读文本\n",
    "# with open('theta.txt', 'w') as f:\n",
    "#     for row in theta:\n",
    "#         f.write(' '.join(map(str, row.tolist())) + '\\n')\n",
    "\n",
    "# 保存 x（形状 [2000, 18]\n",
    "torch.save(x, 'x_3ksamps.pt')\n",
    "# 或分隔符型文本格式\n",
    "# with open('x.txt', 'w') as f:\n",
    "#     for data_point in x:\n",
    "#         f.write(' '.join(f\"{num:.6f}\" for num in data_point.tolist()) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sz_clusters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
